{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.007*\"hes\" + 0.006*\"dad\" + 0.005*\"mom\" + 0.005*\"hey\" + 0.004*\"yeah\" + 0.004*\"years\" + 0.004*\"look\" + 0.004*\"love\" + 0.004*\"shes\" + 0.004*\"tell\"'),\n",
       " (1,\n",
       "  '0.013*\"fucking\" + 0.006*\"yeah\" + 0.005*\"ive\" + 0.005*\"good\" + 0.005*\"cause\" + 0.005*\"love\" + 0.005*\"fuck\" + 0.005*\"better\" + 0.004*\"gonna\" + 0.004*\"didnt\"'),\n",
       " (2,\n",
       "  '0.008*\"gonna\" + 0.007*\"uh\" + 0.006*\"wife\" + 0.005*\"yeah\" + 0.005*\"thing\" + 0.005*\"baby\" + 0.005*\"hes\" + 0.005*\"come\" + 0.005*\"look\" + 0.005*\"ive\"'),\n",
       " (3,\n",
       "  '0.010*\"yeah\" + 0.010*\"shit\" + 0.005*\"black\" + 0.005*\"fuck\" + 0.004*\"come\" + 0.004*\"white\" + 0.004*\"look\" + 0.004*\"guys\" + 0.004*\"gonna\" + 0.004*\"world\"')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#####################################            Topic Modelling on all text             ######################################\n",
    "\n",
    "# import sys\n",
    "# !{sys.executable} -m pip install gensim\n",
    "\n",
    "# import nltk\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "## Importing libraries\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from gensim import matutils, models\n",
    "import scipy.sparse\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "## Loading cleaned data in data-frame form from a pickle file\n",
    "data = pd.read_pickle('data_in_dtm_cleaner.pkl')\n",
    "\n",
    "\n",
    "## Putting the document-term matrix into a new gensim format; from df --> sparse matrix --> gensim corpus\n",
    "dtm = data.transpose()\n",
    "sparse_counts = scipy.sparse.csr_matrix(dtm)\n",
    "corpus = matutils.Sparse2Corpus(sparse_counts)\n",
    "\n",
    "## Creating dictionary of all the terms and finding their in the document-term matrix\n",
    "cv = pickle.load(open(\"cv_with_new_stop-words.pkl\", \"rb\"))\n",
    "id2word = dict((v, k) for k, v in cv.vocabulary_.items())\n",
    "\n",
    "\n",
    "# Now that we have the corpus (term-document matrix) and id2word (dictionary of location: term),\n",
    "# we need to specify two other parameters as well - the number of topics and the number of passes\n",
    "## LDA for num_topics = 2\n",
    "# lda = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=2, passes=10)\n",
    "# lda.print_topics()\n",
    "\n",
    "# ##  LDA for num_topics = 3\n",
    "# lda = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=3, passes=10)\n",
    "# lda.print_topics()\n",
    "\n",
    "# ##  LDA for num_topics = 4\n",
    "# lda = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=4, passes=10)\n",
    "# lda.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.009*\"shes\" + 0.008*\"dad\" + 0.007*\"wife\" + 0.007*\"man\" + 0.006*\"hes\" + 0.006*\"home\" + 0.006*\"house\" + 0.006*\"shit\" + 0.006*\"guy\" + 0.006*\"life\"'),\n",
       " (1,\n",
       "  '0.009*\"hes\" + 0.008*\"thing\" + 0.008*\"wife\" + 0.008*\"man\" + 0.008*\"years\" + 0.007*\"way\" + 0.007*\"cause\" + 0.006*\"guy\" + 0.006*\"life\" + 0.006*\"lot\"'),\n",
       " (2,\n",
       "  '0.011*\"man\" + 0.009*\"shit\" + 0.008*\"years\" + 0.008*\"day\" + 0.007*\"kids\" + 0.007*\"thing\" + 0.007*\"fuck\" + 0.007*\"baby\" + 0.006*\"way\" + 0.005*\"phone\"')]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#####################################            Topic Modelling on nouns only            ######################################\n",
    "\n",
    "## Loading cleaned data in form of raw transcripts\n",
    "data_clean = pd.read_pickle('data_clean.pkl')\n",
    "\n",
    "\n",
    "## Function to tokenize the text and pull out only the nouns\n",
    "def nouns(text):\n",
    "    is_noun = lambda pos: pos[:2] == 'NN'\n",
    "    tokenized = word_tokenize(text)\n",
    "    all_nouns = [word for (word, pos) in pos_tag(tokenized) if is_noun(pos)] \n",
    "    return ' '.join(all_nouns)\n",
    "\n",
    "## Applying the nouns function to the transcripts to filter only on nouns\n",
    "data_nouns = pd.DataFrame(data_clean.Transcript.apply(nouns))\n",
    "\n",
    "\n",
    "## Re-adding the additional stop words since the document-term matrix is being recreated\n",
    "add_stop_words = ['like', 'im', 'know', 'just', 'dont', 'thats', 'right', 'people',\n",
    "                  'youre', 'got', 'gonna', 'time', 'think', 'yeah', 'said']\n",
    "stop_words = text.ENGLISH_STOP_WORDS.union(add_stop_words)\n",
    "\n",
    "## Recreating the document-term matrix with only nouns\n",
    "cvn = CountVectorizer(stop_words=stop_words)\n",
    "data_cvn = cvn.fit_transform(data_nouns.Transcript)\n",
    "data_dtmn = pd.DataFrame(data_cvn.toarray(), columns=cvn.get_feature_names())\n",
    "data_dtmn.index = data_nouns.index\n",
    "\n",
    "\n",
    "## Creating the gensim corpus\n",
    "corpusn = matutils.Sparse2Corpus(scipy.sparse.csr_matrix(data_dtmn.transpose()))\n",
    "\n",
    "## Create the vocabulary dictionary\n",
    "id2wordn = dict((v, k) for k, v in cvn.vocabulary_.items())\n",
    "\n",
    "\n",
    "# ## Applying LDA model with 2 topics\n",
    "# ldan = models.LdaModel(corpus=corpusn, num_topics=2, id2word=id2wordn, passes=10)\n",
    "# ldan.print_topics()\n",
    "# \n",
    "# ## Applying LDA model with 3 topics\n",
    "# ldan = models.LdaModel(corpus=corpusn, num_topics=3, id2word=id2wordn, passes=10)\n",
    "# ldan.print_topics()\n",
    "# \n",
    "# ## Applying LDA model with 4 topics\n",
    "# ldan = models.LdaModel(corpus=corpusn, num_topics=4, id2word=id2wordn, passes=10)\n",
    "# ldan.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.007*\"women\" + 0.007*\"sex\" + 0.005*\"jokes\" + 0.005*\"jen\" + 0.005*\"bit\" + 0.004*\"couch\" + 0.004*\"tension\" + 0.004*\"opinions\" + 0.003*\"moment\" + 0.003*\"joke\"'),\n",
       " (1,\n",
       "  '0.010*\"dad\" + 0.005*\"indian\" + 0.005*\"hasan\" + 0.004*\"wife\" + 0.004*\"horse\" + 0.003*\"brown\" + 0.003*\"kid\" + 0.003*\"college\" + 0.003*\"india\" + 0.003*\"beef\"'),\n",
       " (2,\n",
       "  '0.017*\"wife\" + 0.008*\"uh\" + 0.006*\"black\" + 0.005*\"dad\" + 0.005*\"boy\" + 0.005*\"trump\" + 0.004*\"car\" + 0.004*\"slow\" + 0.004*\"father\" + 0.004*\"bar\"'),\n",
       " (3,\n",
       "  '0.009*\"uh\" + 0.005*\"snoop\" + 0.005*\"son\" + 0.005*\"stuff\" + 0.003*\"frankie\" + 0.003*\"car\" + 0.003*\"person\" + 0.003*\"special\" + 0.003*\"eyes\" + 0.003*\"pussy\"')]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#############################             Topic Modelling on adjectives and nouns only            ##############################\n",
    "\n",
    "## Function to tokenize the text and pull out only the adjectives and nouns\n",
    "def nouns_adj(text):\n",
    "    is_noun_adj = lambda pos: pos[:2] == 'NN' or pos[:2] == 'JJ'\n",
    "    tokenized = word_tokenize(text)\n",
    "    nouns_adj = [word for (word, pos) in pos_tag(tokenized) if is_noun_adj(pos)] \n",
    "    return ' '.join(nouns_adj)\n",
    "\n",
    "## Applying the nouns_adj function to the transcripts to filter only on adjectives and nouns\n",
    "data_nouns_adj = pd.DataFrame(data_clean.Transcript.apply(nouns_adj))\n",
    "\n",
    "\n",
    "## Creating a new document-term matrix using only adjectives and nouns, also removing common words with max_df\n",
    "cvna = CountVectorizer(stop_words=stop_words, max_df=.8)\n",
    "data_cvna = cvna.fit_transform(data_nouns_adj.Transcript)\n",
    "data_dtmna = pd.DataFrame(data_cvna.toarray(), columns=cvna.get_feature_names())\n",
    "data_dtmna.index = data_nouns_adj.index\n",
    "\n",
    "## Creating the gensim corpus\n",
    "corpusna = matutils.Sparse2Corpus(scipy.sparse.csr_matrix(data_dtmna.transpose()))\n",
    "\n",
    "## Creating the vocabulary dictionary\n",
    "id2wordna = dict((v, k) for k, v in cvna.vocabulary_.items())\n",
    "\n",
    "# ## Applying LDA model with 2 topics\n",
    "# ldana = models.LdaModel(corpus=corpusna, num_topics=2, id2word=id2wordna, passes=10)\n",
    "# ldana.print_topics()\n",
    "# \n",
    "# ## Applying LDA model with 3 topics\n",
    "# ldana = models.LdaModel(corpus=corpusna, num_topics=3, id2word=id2wordna, passes=10)\n",
    "# ldana.print_topics()\n",
    "# \n",
    "# ## Applying LDA model with 4 topics\n",
    "# ldana = models.LdaModel(corpus=corpusna, num_topics=4, id2word=id2wordna, passes=10)\n",
    "# ldana.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.011*\"dad\" + 0.006*\"hasan\" + 0.006*\"indian\" + 0.004*\"brown\" + 0.004*\"beef\" + 0.004*\"india\" + 0.003*\"country\" + 0.003*\"dream\" + 0.003*\"movie\" + 0.003*\"bike\"'),\n",
       " (1,\n",
       "  '0.014*\"wife\" + 0.007*\"dad\" + 0.005*\"son\" + 0.005*\"car\" + 0.005*\"uh\" + 0.004*\"snoop\" + 0.004*\"boy\" + 0.004*\"kid\" + 0.003*\"bar\" + 0.003*\"slow\"'),\n",
       " (2,\n",
       "  '0.007*\"women\" + 0.006*\"sex\" + 0.005*\"jokes\" + 0.004*\"jen\" + 0.004*\"bit\" + 0.004*\"couch\" + 0.003*\"opinions\" + 0.003*\"tension\" + 0.003*\"joke\" + 0.003*\"moment\"'),\n",
       " (3,\n",
       "  '0.017*\"uh\" + 0.010*\"black\" + 0.006*\"stuff\" + 0.004*\"eyes\" + 0.004*\"trump\" + 0.004*\"vicks\" + 0.004*\"dan\" + 0.004*\"grandma\" + 0.003*\"person\" + 0.003*\"bit\"')]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#######################################             Final Topic Modelling            ##########################################\n",
    "\n",
    "## (Changing the num_topics and passes parameters can yield better results)\n",
    "ldana = models.LdaModel(corpus=corpusna, num_topics=4, id2word=id2wordna, passes=70)\n",
    "ldana.print_topics()\n",
    "\n",
    "# Topic 0: (dad, India, movie)\n",
    "# Topic 1: (wife, family, car)\n",
    "# Topic 2: (women, sex, jokes)\n",
    "# Topic 3: (black, trump)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 'Adam Sandler'),\n",
       " (2, 'Ali Wong'),\n",
       " (3, 'Aziz Ansari'),\n",
       " (2, 'Daniel Sloss'),\n",
       " (1, 'Gabriel “Fluffy” Iglesias'),\n",
       " (2, 'Hannah Gadsby'),\n",
       " (0, 'Hasan Minhaj'),\n",
       " (1, 'John Mulaney'),\n",
       " (2, 'Mike Birbiglia'),\n",
       " (1, 'Sebastian Maniscalco'),\n",
       " (1, 'Seth Meyers'),\n",
       " (0, 'Vir Das'),\n",
       " (3, 'Wanda Sykes')]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "###############################             Final Topic allocation to the comedians            #################################\n",
    "\n",
    "## Checking which transcripts cover which topics\n",
    "corpus_transformed = ldana[corpusna]\n",
    "list(zip([a for [(a,b)] in corpus_transformed], data_dtmna.index))\n",
    "\n",
    "# Topic 0: (dad, India, movie)   [Hasan Minhaj, Vir Das]\n",
    "# Topic 1: (wife, family, car)   [Adam Sandler, Gabriel Iglesias, John Mulaney, Sebastian Maniscalco, Seth Meyers]\n",
    "# Topic 2: (women, sex, jokes)   [Ali Wong, Daniel Sloss, Hannah Gadsby, Mike Birbiglia]\n",
    "# Topic 3: (black, trump)        [Aziz Ansari, Wanda Sykes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
